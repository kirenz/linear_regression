
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Regression diagnostics &#8212; Introduction to Regression Models</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Regression Models</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear regression
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="basics.html">
   Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="diagnostics.html">
   Regression diagnostics
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lasso, Splines &amp; GAM
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="hitters_data.html">
   Hitters data preparation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lasso.html">
   Lasso regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="splines.html">
   Regression splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gam.html">
   Generalized Additive Models (GAM)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Decision Trees
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="randomforest.html">
   Random forest in scikit-learn
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradientboosting.html">
   Gradient Boosting in scikit-learn
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="xgboost-regression.html">
   XGBoost
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradientboosting-xgboost.html">
   Gradient Boosting with XGBoost
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Combine predictors
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="ensemble.html">
   Ensemble meta-estimator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stacking.html">
   Stacking
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Case Duke
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="case-duke.html">
   Case study
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="case-duke-exploration.html">
   Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="case-duke-data-prep-template.html">
   Create data prep file
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="case-duke-statsmodel.html">
   Statsmodels
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="case-duke-sklearn.html">
   Scikit-learn
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Case Happy
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="ds-happy-stats.html">
   Simple statsmodels model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ds-happy-scikit.html">
   Simple scikit learn model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ds-happy-scikit-split.html">
   Linear Regression with scikit-learn
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Case California housing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="case-ca-housing.html">
   Machine Learning project
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="reference.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/kirenz/regression/blob/main/docs/diagnostics-c.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/kirenz/regression"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/kirenz/regression/issues/new?title=Issue%20on%20page%20%2Fdocs/diagnostics-c.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/docs/diagnostics-c.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Regression diagnostics
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#python-setup">
     Python setup
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-data">
     Import data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model">
     Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Regression diagnostics
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#outliers-and-high-leverage-points">
     Outliers and high-leverage points
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#influence-plots">
       Influence plots
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#solutions">
       Solutions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-linearity-and-heteroscedasticity">
     Non-linearity and heteroscedasticity
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#partial-regression-plots">
       Partial Regression Plots
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ccpr-plots">
       CCPR plots
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#residuals-vs-fitted-plot">
       Residuals vs fitted plot
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#breusch-pagan-lagrange-multiplier-test">
       Breusch-Pagan Lagrange Multiplier test
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#single-variable-regression-diagnostics">
       Single Variable Regression Diagnostics
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       Solutions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-normally-distributed-errors">
     Non-normally distributed errors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#jarque-bera-test">
       Jarque-Bera test
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#omnibus-normtest">
       Omnibus normtest
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#solution">
       Solution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#correlation-of-error-terms">
     Correlation of error terms
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#durbin-watson-test">
       Durbin-Watson test
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id11">
       Solution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#collinearity">
     Collinearity
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#correlation-matrix">
       Correlation matrix
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variance-inflation-factor-vif">
       Variance inflation factor (VIF)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id12">
       Solution
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Regression diagnostics</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Regression diagnostics
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#python-setup">
     Python setup
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-data">
     Import data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model">
     Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Regression diagnostics
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#outliers-and-high-leverage-points">
     Outliers and high-leverage points
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#influence-plots">
       Influence plots
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#solutions">
       Solutions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-linearity-and-heteroscedasticity">
     Non-linearity and heteroscedasticity
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#partial-regression-plots">
       Partial Regression Plots
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ccpr-plots">
       CCPR plots
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#residuals-vs-fitted-plot">
       Residuals vs fitted plot
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#breusch-pagan-lagrange-multiplier-test">
       Breusch-Pagan Lagrange Multiplier test
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#single-variable-regression-diagnostics">
       Single Variable Regression Diagnostics
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       Solutions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-normally-distributed-errors">
     Non-normally distributed errors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#jarque-bera-test">
       Jarque-Bera test
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#omnibus-normtest">
       Omnibus normtest
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#solution">
       Solution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#correlation-of-error-terms">
     Correlation of error terms
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#durbin-watson-test">
       Durbin-Watson test
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id11">
       Solution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#collinearity">
     Collinearity
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#correlation-matrix">
       Correlation matrix
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variance-inflation-factor-vif">
       Variance inflation factor (VIF)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id12">
       Solution
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="regression-diagnostics">
<h1>Regression diagnostics<a class="headerlink" href="#regression-diagnostics" title="Permalink to this headline">#</a></h1>
<p>When we fit a linear regression model to a particular data set, many problems may occur. Most common among these are the following <span id="id1">[<a class="reference internal" href="reference.html#id7" title="Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical learning. Springer, 2021. URL: https://www.statlearning.com/.">James <em>et al.</em>, 2021</a>]</span>:</p>
<ol class="simple">
<li><p>Outliers and high-leverage points</p></li>
<li><p>Non-linearity of the response-predictor relationship</p></li>
<li><p>Non-constant variance of error terms (heteroscedasticity)</p></li>
<li><p>Non-normally distributed errors</p></li>
<li><p>Correlation of error terms</p></li>
<li><p>Collinearity</p></li>
</ol>
<p>As an example of how to identify these problems, we use data about the prestige of canadian occupations to estimate a regression model with <code class="docutils literal notranslate"><span class="pre">prestige</span></code> as the response and <code class="docutils literal notranslate"><span class="pre">income</span></code> and <code class="docutils literal notranslate"><span class="pre">education</span></code> as predictors. Review this <a class="reference external" href="https://vincentarelbundock.github.io/Rdatasets/doc/carData/Prestige.html">site</a> to learn more about the data.</p>
<p><em>This tutorial is mainly based on the excellent <a class="reference external" href="https://www.statsmodels.org/dev/examples/notebooks/generated/regression_plots.html#Partial-Regression-Plots-(Crime-Data)">statmodels documentation</a> about regression plots.</em></p>
<section id="python-setup">
<h2>Python setup<a class="headerlink" href="#python-setup" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">patsy</span> <span class="kn">import</span> <span class="n">dmatrices</span>

<span class="kn">from</span> <span class="nn">statsmodels.compat</span> <span class="kn">import</span> <span class="n">lzip</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">import</span> <span class="n">ols</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>
<span class="kn">from</span> <span class="nn">statsmodels.tools.tools</span> <span class="kn">import</span> <span class="n">add_constant</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s2">&quot;figure&quot;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s2">&quot;font&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="import-data">
<h2>Import data<a class="headerlink" href="#import-data" title="Permalink to this headline">#</a></h2>
<p>We use the function <code class="docutils literal notranslate"><span class="pre">.datasets.get_rdataset</span></code> to load the dataset from the <a href="https://vincentarelbundock.github.io/Rdatasets/">Rdatasets package</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">get_rdataset</span><span class="p">(</span><span class="s2">&quot;Duncan&quot;</span><span class="p">,</span> <span class="s2">&quot;carData&quot;</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show head of DataFrame</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot numerical data as pairs</span>
<span class="n">sns</span><span class="o">.</span><span class="n">___</span><span class="p">(</span><span class="n">df</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Findings:</p>
<ul class="simple">
<li><p>Positive association between prestige and eduction as well as between prestige and income</p></li>
<li><p>Relationships seem to be linear in both cases.</p></li>
</ul>
</section>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># estimate the model and save it as lm (linear model)</span>
<span class="c1"># dependent: prestige; features: income and education, data = df</span>
<span class="n">lm</span> <span class="o">=</span> <span class="n">ols</span><span class="p">(</span><span class="s2">&quot;___ ~ ___ + ___&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">___</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print regression results with .summary()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">___</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id2">
<h1>Regression diagnostics<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h1>
<section id="outliers-and-high-leverage-points">
<h2>Outliers and high-leverage points<a class="headerlink" href="#outliers-and-high-leverage-points" title="Permalink to this headline">#</a></h2>
<p>In a regression analysis, single observations can have a strong influence on the results of the model.</p>
<p>For example, in the plot below we can see how a single outlying data point can affect a model.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># HIDE CODE</span>
<span class="n">df_outlier</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="p">{</span> <span class="s1">&#39;observation&#39;</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">([</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">,</span> <span class="s2">&quot;E&quot;</span> <span class="p">]),</span>
          <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">),</span>
          <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)}</span>
        <span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df_outlier</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>We can spot outliers by using graphs like the scatterplot above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">___</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>We just saw that outliers are observations for which the response <span class="math notranslate nohighlight">\(y_i\)</span> is unusual given the predictor <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<p>In contrast, observations with high <strong>leverage</strong> have an unusual value for <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># HIDE CODE</span>
<span class="n">df_leverage</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="p">{</span> <span class="s1">&#39;observation&#39;</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">([</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">,</span> <span class="s2">&quot;E&quot;</span> <span class="p">]),</span>
          <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">),</span>
          <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)}</span>
        <span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;__&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;___&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df_leverage</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>For example, the observation with a value of <span class="math notranslate nohighlight">\(x=20\)</span> has high leverage, in that the predictor value for this observation is large relative to the other observations. The removal of the high leverage observation would have a substantial impact on the regression line. In general, high leverage observations tend to have a sizable impact on the estimated regression line. Therefore, it is important to detect influential observations and to take them into consideration when interpreting the results.</p>
<p>Again, we could use simple graphs to identify unusual observations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">___</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Boxplot</span>
<span class="n">sns</span><span class="o">.</span><span class="n">___</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df_leverage</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<section id="influence-plots">
<h3>Influence plots<a class="headerlink" href="#influence-plots" title="Permalink to this headline">#</a></h3>
<blockquote>
<div><p>Influence plots are used to identify influential data points.</p>
</div></blockquote>
<blockquote>
<div><p>They depend on both the residual and leverage i.e they take into account both the <span class="math notranslate nohighlight">\(x\)</span> value and <span class="math notranslate nohighlight">\(y\)</span> value of the observation.</p>
</div></blockquote>
<p>We can use <strong>influence plots</strong> to identify observations in our independent variables which have “unusual” values in comparison to other values.</p>
<p>Influence plots show the (externally) studentized residuals vs. the leverage of each observation:</p>
<p>Dividing a statistic by a sample standard deviation is called <strong>studentizing</strong>, in analogy with standardizing and normalizing. The basic idea is to:</p>
<ol class="simple">
<li><p>Delete the observations one at a time.</p></li>
<li><p>Refit the regression model each time on the remaining n–1 observations.</p></li>
<li><p>Compare the observed response values to their fitted values based on the models with the ith observation deleted. This produces unstandardized deleted residuals.</p></li>
<li><p>Standardising the deleted residuals produces studentized deleted residuals (also known as externally studentized residuals)</p></li>
</ol>
<p>In essence, externally studentized residuals are residuals that are scaled by their standard deviation. If an observation has an externally studentized residual that is larger than 3 (in absolute value) we can call it an outlier. However, values greater then 2 (in absolute values) are usually also of interest.</p>
<p><strong>Leverage</strong> is a measure of how far away the independent variable values of an observation are from those of the other observations. High-leverage points are outliers with respect to the independent variables.</p>
<p>In statsmodels <code class="docutils literal notranslate"><span class="pre">.influence_plot</span></code> the influence of each point can be visualized by the <code class="docutils literal notranslate"><span class="pre">criterion</span></code> keyword argument. Options are Cook’s distance and DFFITS, two measures of influence. Steps to compute Cook’s distance:</p>
<ol class="simple">
<li><p>Delete observations one at a time.</p></li>
<li><p>Refit the regression model on remaining (n−1) observations</p></li>
<li><p>Examine how much all of the fitted values change when the ith observation is deleted.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">influence_plot</span><span class="p">(</span><span class="n">___</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;cooks&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To identify values with high influence, we look for observations with:</p>
<ol class="simple">
<li><p>big blue points (high Cook’s distance) and</p></li>
<li><p>high leverage (X-axis) which additionally have</p></li>
<li><p>high or low studentized residuals (Y-axis).</p></li>
</ol>
<p>There are a few worrisome observations with big blue dots in the plot:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">RR.engineer</span></code> has large leverage but small residual.</p></li>
<li><p>Both <code class="docutils literal notranslate"><span class="pre">contractor</span></code> and <code class="docutils literal notranslate"><span class="pre">reporter</span></code> have low leverage but a large residual.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Conductor</span></code> and <code class="docutils literal notranslate"><span class="pre">minister</span></code> have both high leverage and large residuals, and, therefore, large influence.</p></li>
</ul>
<p>A general rule of thumb is that observations with a <strong>Cook’s distance</strong> over <span class="math notranslate nohighlight">\(4/n\)</span> (where n is the number of observations) are possible outliers with leverage.</p>
<p>In addition to our plot, we can use the function <code class="docutils literal notranslate"><span class="pre">.get_influence()</span></code> to assess the influence of each observation and compare them to the cricital Cook’s distance :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># obtain Cook&#39;s distance </span>
<span class="c1"># get_influence()</span>
<span class="n">lm_cooksd</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">___</span><span class="p">()</span><span class="o">.</span><span class="n">cooks_distance</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># get length of df to obtain n</span>
<span class="c1"># len</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">___</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;income&quot;</span><span class="p">])</span>

<span class="c1"># calculate critical d</span>
<span class="n">critical_d</span> <span class="o">=</span> <span class="mi">4</span><span class="o">/</span><span class="n">n</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Critical Cooks distance:&#39;</span><span class="p">,</span> <span class="n">critical_d</span><span class="p">)</span>

<span class="c1"># identification of potential outliers with leverage</span>
<span class="n">out_d</span> <span class="o">=</span> <span class="n">lm_cooksd</span> <span class="o">&gt;</span> <span class="n">___</span>

<span class="c1"># output potential outliers with leverage</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">out_d</span><span class="p">],</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> 
    <span class="n">lm_cooksd</span><span class="p">[</span><span class="n">out_d</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="solutions">
<h3>Solutions<a class="headerlink" href="#solutions" title="Permalink to this headline">#</a></h3>
<p>If you have unusual observations in your data (unusual depends on your use case and domain knowledge) here some approaches to deal with them:</p>
<ol class="simple">
<li><p><strong>Drop</strong> the identified unusual observations.</p></li>
<li><p>Analyse your data with <strong>robust methods</strong> like bootstrapping (sampling with replacement).</p></li>
<li><p><strong>Trim</strong> the data (delete a certain amount of scores from the extremes)</p></li>
<li><p><strong>Windsorizing</strong> (substitute outliers with the highest value that isn’t an outlier)</p></li>
<li><p><strong>Transform</strong> the data (by applying a mathematical function to scores - like a log transformation to reduce positive skew).</p></li>
</ol>
<p>A note on data transformations:</p>
<p>Be aware that transforming the data helps as often as it hinders model performance (Games &amp; Lucas, 1966). According to Games (1984):</p>
<ul class="simple">
<li><p>Because of the central limit theorem, the sampling distribution will be normal in samples &gt; 40 anyway.</p></li>
<li><p>Transforming the data changes the hypothesis being tested (e.g. when using a log transformation and comparing means you change from comparing arithmetic means to comparing geometric means)</p></li>
<li><p>In small samples it is tricky to determine normality one way or another.</p></li>
<li><p>The consequences for the statistical model of applying the “wrong” transformation could be worse than the consequences of analysing the untransformed scores.</p></li>
</ul>
</section>
</section>
<section id="non-linearity-and-heteroscedasticity">
<h2>Non-linearity and heteroscedasticity<a class="headerlink" href="#non-linearity-and-heteroscedasticity" title="Permalink to this headline">#</a></h2>
<p>One crucial assumption of the linear regression model is the linear relationship between the response and the dependent variables. We can identify non-linear relationships in the regression model residuals if the residuals are not equally spread around the horizontal line (where the residuals are zero) but instead show a pattern, then this gives us an indication for a non-linear relationship.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We can deal with non-linear relationships via basis expansions (e.g. polynomial regression) or regression splines (<a class="reference external" href="http://www.feat.engineering/numeric-one-to-many.html#numeric-basis-functions">see <span id="id3">Kuhn and Johnson [<a class="reference internal" href="reference.html#id5" title="Max Kuhn and Kjell Johnson. Feature engineering and selection: A practical approach for predictive models. CRC Press, 2019. URL: http://www.feat.engineering/.">2019</a>]</span></a>)</p>
</div>
<p>Another important assumption is that the error terms have a constant variance (homoscedasticity). For instance, the variances of the error terms may increase with the value of the response. One can identify non-constant variances in the errors, or heteroscedasticity, from the presence of a funnel shape in a residual plot.</p>
<p>When faced with this problem, one possible solution is to use <a class="reference external" href="https://www.statsmodels.org/dev/examples/notebooks/generated/wls.html">weighted regression</a>. This type of regression assigns a weight to each data point based on the variance of its fitted value. Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.</p>
<section id="partial-regression-plots">
<h3>Partial Regression Plots<a class="headerlink" href="#partial-regression-plots" title="Permalink to this headline">#</a></h3>
<p>Since we are doing multivariate regressions, we cannot just look at individual bivariate plots (e.g., prestige and income) to identify the type of relationships between response and predictor.</p>
<p>Instead, we want to look at the relationship of the dependent variable and independent variables <em>conditional</em> on the other independent variables. We can do this through using <strong>partial regression plots</strong>, otherwise known as <strong>added variable plots</strong>.</p>
<p>With partial regression plots you can:</p>
<ol class="simple">
<li><p>Investigate the relationship between a dependent and independent variables conditional on other independent varibales.</p></li>
<li><p>Identify the effects of the individual data values on the estimation of a coefficient.</p></li>
<li><p>Investigate violations of underlying assumptions such as linearity and homoscedasticity.</p></li>
</ol>
<p>If we want to identify the relationship between <code class="docutils literal notranslate"><span class="pre">prestige</span></code> and <code class="docutils literal notranslate"><span class="pre">income</span></code>, we would proceed as follows (we name the independent variable of interest <span class="math notranslate nohighlight">\(X_k\)</span> and all other independent variables <span class="math notranslate nohighlight">\(X_{\sim k}\)</span>):</p>
<br/>
<ol class="simple">
<li><p>Compute a regression model by regressing the response variable versus the independent variables excluding <span class="math notranslate nohighlight">\(X_k\)</span>:</p>
<ul class="simple">
<li><p>response variable: <code class="docutils literal notranslate"><span class="pre">prestige</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(X_k\)</span>: <code class="docutils literal notranslate"><span class="pre">income</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(X_{\sim k}\)</span>: <code class="docutils literal notranslate"><span class="pre">education</span></code></p></li>
<li><p>Model(<span class="math notranslate nohighlight">\(X_{\sim k}\)</span>): <code class="docutils literal notranslate"><span class="pre">(prestige</span> <span class="pre">~</span> <span class="pre">education)</span></code></p></li>
</ul>
</li>
</ol>
<br/>
<ol class="simple">
<li><p>Compute the residuals of Model(<span class="math notranslate nohighlight">\(X_{\sim k}\)</span>):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R_{X_{\sim k}}\)</span>: residuals of Model(<span class="math notranslate nohighlight">\(X_{\sim k}\)</span>):</p></li>
</ul>
</li>
</ol>
<br/>
<ol class="simple">
<li><p>Compute a new regression model by regressing <span class="math notranslate nohighlight">\(R_{X_{\sim k}}\)</span> on <span class="math notranslate nohighlight">\(X_{\sim k}\)</span>.</p>
<ul class="simple">
<li><p>Model(<span class="math notranslate nohighlight">\(X_k\)</span>): <span class="math notranslate nohighlight">\(R_{X_{\sim k}}\)</span> ~ <code class="docutils literal notranslate"><span class="pre">income</span></code></p></li>
</ul>
</li>
</ol>
<br/>
<ol class="simple">
<li><p>Compute the residuals of Model(<span class="math notranslate nohighlight">\(X_k\)</span>):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R_{X_{k}}\)</span>: residuals of Model(<span class="math notranslate nohighlight">\(X_{k}\)</span>):</p></li>
</ul>
</li>
</ol>
<br/>
<ol class="simple">
<li><p>Make a partial regression plot by plotting the residuals from <span class="math notranslate nohighlight">\(R_{X_{\sim k}}\)</span> against the residuals from <span class="math notranslate nohighlight">\(R_{X_{k}}\)</span>:</p>
<ul class="simple">
<li><p>Plot with X = <span class="math notranslate nohighlight">\(R_{X_{k}}\)</span> and Y = <span class="math notranslate nohighlight">\(R_{X_{\sim k}}\)</span></p></li>
</ul>
</li>
</ol>
<br/>
<p>For a quick check of all the regressors, you can use <code class="docutils literal notranslate"><span class="pre">plot_partregress_grid</span></code>. These plots will not label the
points, but you can use them to identify problems and then use <code class="docutils literal notranslate"><span class="pre">plot_partregress</span></code> to get more information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_partregress_grid</span><span class="p">(</span><span class="n">lm</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We observe a positive relationship between prestige and income as well as between prestige and education. The relationship seems to be linear in both cases.</p>
<p>Next, let’s take a closer look at the observations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># feature of interest: exog_i = income; exog_others = education</span>

<span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_partregress</span><span class="p">(</span>
                             <span class="n">endog</span><span class="o">=</span><span class="s1">&#39;___&#39;</span><span class="p">,</span> <span class="c1"># response</span>
                             <span class="n">exog_i</span><span class="o">=</span><span class="s1">&#39;income&#39;</span><span class="p">,</span> <span class="c1"># variable of interest</span>
                             <span class="n">exog_others</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;___&#39;</span><span class="p">],</span> <span class="c1"># other predictors</span>
                             <span class="n">data</span><span class="o">=</span><span class="n">___</span><span class="p">,</span>  <span class="c1"># dataframe</span>
                             <span class="n">obs_labels</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># show labels</span>
                             <span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># same plot for education</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_partregress</span><span class="p">(</span><span class="s2">&quot;prestige&quot;</span><span class="p">,</span> <span class="s2">&quot;education&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;income&quot;</span><span class="p">],</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The partial regression plots confirm the influence of <code class="docutils literal notranslate"><span class="pre">conductor</span></code>, <code class="docutils literal notranslate"><span class="pre">minister</span></code> on the partial relationships between prestige and the predictors income and education. The influence of <code class="docutils literal notranslate"><span class="pre">reporter</span></code> is less obvious but since the observation was considered critical according to Cook`s D, we also drop this case.</p>
<p>Note that influential cases potentially bias the effect of income and education on prestige. Therefore, we will drop these cases and perform a linear regression without them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># make a subset and flag as TRUE if index doesn&#39;t contain ... use ~</span>
<span class="n">subset</span> <span class="o">=</span> <span class="n">__df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="s2">&quot;conductor&quot;</span><span class="p">,</span> <span class="s2">&quot;minister&quot;</span><span class="p">,</span> <span class="s2">&quot;reporter&quot;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">subset</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compute regression without influential cases</span>
<span class="n">lm2</span> <span class="o">=</span> <span class="n">ols</span><span class="p">(</span><span class="s2">&quot;prestige ~ income + education&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="n">___</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">lm2</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Dropping the influential cases confirms our believe. The new regression model without the influential cases is superior to the initial one. To see this, compare the regression summary above with our initial summary (e.g., see <span class="math notranslate nohighlight">\(R^2\)</span>, adjusted <span class="math notranslate nohighlight">\(R^2\)</span> and the F-statistic).</p>
</section>
<section id="ccpr-plots">
<h3>CCPR plots<a class="headerlink" href="#ccpr-plots" title="Permalink to this headline">#</a></h3>
<p>The Component-Component plus Residual (CCPR) plot provides another way to judge the effect of one regressor on the response variable by taking into account the effects of the other independent variables. They are also a good way to see if the predictors have a linear relationship with the dependent variable.</p>
<p>A CCPR plot consists of a partial residuals plot and a component:</p>
<ol class="simple">
<li><p>The partial residuals plot is defined as <span class="math notranslate nohighlight">\(\text{Residuals} + \hat\beta_{i} X_i \text{ }\text{ }\)</span> versus <span class="math notranslate nohighlight">\(X_i\)</span>, where</p>
<ul class="simple">
<li><p>Residuals = residuals from the full model</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat\beta_{i}\)</span> = regression coefficient from the ith independent variable in the full model</p></li>
<li><p><span class="math notranslate nohighlight">\(X_i\)</span> = the ith independent variable</p></li>
</ul>
</li>
<li><p>The component adds <span class="math notranslate nohighlight">\(\hat\beta_{i}\)</span> <span class="math notranslate nohighlight">\(X_i\)</span> versus <span class="math notranslate nohighlight">\(X_i\)</span> to show where the fitted line would lie.</p></li>
</ol>
<p>A significant difference between the residual line and the actual distribution of values indicates that the predictor does not have a linear relationship with the dependent variable.</p>
<p><em>Care should be taken if <span class="math notranslate nohighlight">\(X_i\)</span> is highly correlated with any of the other independent variables (see multicollinearity). If this is the case, the variance evident in the plot will be an underestimate of the true variance.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_ccpr</span><span class="p">(</span><span class="n">lm</span><span class="p">,</span> <span class="s2">&quot;education&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As you can see the relationship between the variation in <span class="math notranslate nohighlight">\(y\)</span> explained by education conditional on income seems to be linear, though you can see there are some observations that are exerting considerable influence on the relationship (see section leverage).</p>
<p>We can quickly look at more than one variable by using <code class="docutils literal notranslate"><span class="pre">plot_ccpr_grid</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_ccpr_grid</span><span class="p">(</span><span class="n">lm</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="residuals-vs-fitted-plot">
<h3>Residuals vs fitted plot<a class="headerlink" href="#residuals-vs-fitted-plot" title="Permalink to this headline">#</a></h3>
<p>Residual plots are a useful graphical tool for identifying non-linearity as well as heteroscedasticity. The residuals of this plot are those of the regression fit with all predictors.</p>
<p>You can use <a class="reference external" href="https://seaborn.pydata.org/generated/seaborn.residplot.html">seaborn’s residplot</a> to investigate possible violations of underlying assumptions such as linearity and homoskedasticity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fitted values</span>
<span class="n">model_fitted_y</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">fittedvalues</span>

<span class="c1">#  Plot</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">___</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">___</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;___&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">lowess</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                     <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">},</span> 
                     <span class="n">line_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">})</span>

<span class="c1"># Titel and labels</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Residuals vs Fitted&#39;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Fitted values&#39;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>This plot shows how residuals are spread along the ranges of predictors. It’s a good sign if we observe relatively equal (randomly) spreaded points along the dotted horizontal line where the residuals are zero.</p>
<p>The red line indicates the fit of a locally weighted scatterplot smoothing (lowess), a local regression method, to the residual scatterplot. <strong>Local regression</strong> is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point <span class="math notranslate nohighlight">\(x_0\)</span> using only the nearby training observations {see cite:t}<code class="docutils literal notranslate"><span class="pre">James2021</span></code> for more details).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>lowess creates a smooth line through the scatter plot to help us investigate the relationship between the fitted values and the residuals.</p>
</div>
<p>We can interpret the result of our lowess fit as follows: The fit is almost equal to the dotted horizontal line where the residuals are zero. This is an indication for a linear relationship. Note that in the case of non-linear relationships, we typically observe a pattern which deviates strongly from a horizontal line.</p>
<p>Regarding <strong>homoscedasticity</strong>, the residuals seem to spread equally wide with an increase of x. This is an indication of homoscedasticiy. Next, we use the Breusch-Pagan Lagrange Multiplier test to confirm our assumption.</p>
</section>
<section id="breusch-pagan-lagrange-multiplier-test">
<h3>Breusch-Pagan Lagrange Multiplier test<a class="headerlink" href="#breusch-pagan-lagrange-multiplier-test" title="Permalink to this headline">#</a></h3>
<blockquote>
<div><p>The Breusch-Pagan Lagrange Multiplier test can be used to identify heteroscedasticity.</p>
</div></blockquote>
<ul class="simple">
<li><p>The test assumes homoscedasticity (this is the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>)</p></li>
<li><p>This means that the residual variance does not depend on the values of the variables in x.</p></li>
</ul>
<p><em>Note that this test may exaggerate the significance of results in small or moderately large samples <span id="id4">[<a class="reference internal" href="reference.html#id2" title="William H Greene. Econometric analysis 4th edition. International edition, New Jersey: Prentice Hall, pages 201–215, 2000.">Greene, 2000</a>]</span>. In this case the F-statistic is preferable.</em></p>
<ul class="simple">
<li><p>If one of the test statistics is significant (i.e., p &lt;= 0.05), then you have indication of heteroscedasticity.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">name</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Lagrange multiplier statistic&#39;</span><span class="p">,</span> <span class="s1">&#39;p-value&#39;</span><span class="p">,</span> <span class="s1">&#39;f-value&#39;</span><span class="p">,</span> <span class="s1">&#39;f p-value&#39;</span><span class="p">]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">het_breuschpagan</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">resid</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">exog</span><span class="p">)</span>
<span class="n">lzip</span><span class="p">(</span><span class="n">___</span><span class="p">,</span> <span class="n">___</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>In our case, both p-values are above 0.05,</p></li>
<li><p>This means we ____ the null hypothesis.</p></li>
<li><p>Therefore, we have an indication of _______.</p></li>
</ul>
</section>
<section id="single-variable-regression-diagnostics">
<h3>Single Variable Regression Diagnostics<a class="headerlink" href="#single-variable-regression-diagnostics" title="Permalink to this headline">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">plot_regress_exog</span></code> function is a convenience function which can be used for quickly checking modeling assumptions with respect to a single regressor.</p>
<p>It gives a 2x2 plot containing the:</p>
<ol class="simple">
<li><p>dependent variable and fitted values with prediction confidence intervals vs. the independent variable chosen,</p></li>
<li><p>the residuals of the model vs. the chosen independent variable,</p></li>
<li><p>a partial regression plot, and a</p></li>
<li><p>CCPR plot.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_regress_exog</span><span class="p">(</span><span class="n">lm</span><span class="p">,</span> <span class="s2">&quot;income&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id5">
<h3>Solutions<a class="headerlink" href="#id5" title="Permalink to this headline">#</a></h3>
<p>If we are faced with non-linear relationships between the dependent and independent variables, we can adjust the linear model via different methods (for more details, <a class="reference external" href="http://www.feat.engineering/numeric-one-to-many.html">see <span id="id6">Kuhn and Johnson [<a class="reference internal" href="reference.html#id5" title="Max Kuhn and Kjell Johnson. Feature engineering and selection: A practical approach for predictive models. CRC Press, 2019. URL: http://www.feat.engineering/.">2019</a>]</span></a>):</p>
<ul class="simple">
<li><p>Basis expansions (polynomial features)</p></li>
<li><p>Regression splines (especially natural cubic splines)</p></li>
<li><p>Locally weighted scatterplot smoothing (loess)</p></li>
<li><p>Generalized additive models (GAMs)</p></li>
</ul>
<p>In particular, GAMs extend general linear models to have nonlinear terms for individual predictors. GAM models can adaptively model separate basis functions for different variables and estimate the complexity for each. In other words, different predictors can be modeled with different levels of complexity <span id="id7">[<a class="reference internal" href="reference.html#id5" title="Max Kuhn and Kjell Johnson. Feature engineering and selection: A practical approach for predictive models. CRC Press, 2019. URL: http://www.feat.engineering/.">Kuhn and Johnson, 2019</a>]</span>.</p>
<p>In case of heteroscedasticity, you have the following options to get realsitic estimates of the standard errors:</p>
<ul class="simple">
<li><p>Use heteroskedasticity-consistent standard error estimators for OLS regression (see <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.get_robustcov_results.html">statsmodels</a></p></li>
<li><p>Use weight least squares (see <a class="reference external" href="https://www.statsmodels.org/dev/examples/notebooks/generated/wls.html">statsmodels</a>)</p></li>
</ul>
</section>
</section>
<section id="non-normally-distributed-errors">
<h2>Non-normally distributed errors<a class="headerlink" href="#non-normally-distributed-errors" title="Permalink to this headline">#</a></h2>
<p>It can be helpful if the residuals in the model are random, normally distributed variables with a mean of 0. This assumption means that the differences between the predicted and observed data are most frequently zero or very close to zero, and that differences much greater than zero happen only occasionally.</p>
<p>Note that non-normally distributed errors are not problematic for our model parameters but they may effect significance tests and confidence intervals.</p>
<section id="jarque-bera-test">
<h3>Jarque-Bera test<a class="headerlink" href="#jarque-bera-test" title="Permalink to this headline">#</a></h3>
<p>The Jarque–Bera (JB) test is a goodness-of-fit test of whether sample data have the <a class="reference external" href="https://en.wikipedia.org/wiki/Skewness"><em>skewness</em></a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Kurtosis"><em>kurtosis</em></a> matching a normal distribution.</p>
<p>The null hypothesis (<span class="math notranslate nohighlight">\(H_0\)</span>) is a joint hypothesis of:</p>
<ol class="simple">
<li><p>the skewness being zero and</p></li>
<li><p>the kurtosis being 3.</p></li>
</ol>
<ul class="simple">
<li><p>This means normal distribution</p></li>
</ul>
<p>Samples from a normal distribution have an:</p>
<ol class="simple">
<li><p>expected skewness of 0 and</p></li>
<li><p>an expected <em>excess</em> kurtosis of 0 (which is the same as a kurtosis of 3).</p></li>
</ol>
<p>Any deviation from this assumptions increases the JB statistic.</p>
<p>Next, we calculate the statistics but you can also find the results of the Jarque-Bera test in the regression summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">name</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Jarque-Bera&#39;</span><span class="p">,</span> <span class="s1">&#39;Chi^2 two-tail prob.&#39;</span><span class="p">,</span> <span class="s1">&#39;Skew&#39;</span><span class="p">,</span> <span class="s1">&#39;Kurtosis&#39;</span><span class="p">]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">jarque_bera</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>

<span class="n">lzip</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The p-value is above 0.05 and we can ____ <span class="math notranslate nohighlight">\(H_0\)</span>.</p></li>
<li><p>Therefore, the test gives us an indication that the errors are ______ distributed.</p></li>
</ul>
</section>
<section id="omnibus-normtest">
<h3>Omnibus normtest<a class="headerlink" href="#omnibus-normtest" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Another test for normal distribution of residuals is the Omnibus normtest (also included in the regression summary).</p></li>
<li><p>The test allows us to check whether or not the model residuals follow an approximately normal distribution.</p></li>
</ul>
<ul class="simple">
<li><p>Our null hypothesis (<span class="math notranslate nohighlight">\(H_0\)</span>) is that the residuals are from a normal distribution.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">name</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Chi^2&#39;</span><span class="p">,</span> <span class="s1">&#39;Two-tail probability&#39;</span><span class="p">]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">omni_normtest</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>
<span class="n">lzip</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The p-value is above 0.05 and we can _____ <span class="math notranslate nohighlight">\(H_0\)</span>.</p></li>
<li><p>Therefore, the test gives us an indication that the errors are from a _____ distribution.</p></li>
</ul>
</section>
<section id="solution">
<h3>Solution<a class="headerlink" href="#solution" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Note that the assumption of normality can be (at least partly) relaxed if the sample size N is large enough; the errors need not follow a normal distribution because of the <a class="reference external" href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem (CLT)</a>.</p></li>
<li><p>In other words, the assumption of normality is in most of the cases not crucial with large enough N (usually if N ≥ 50).</p></li>
</ul>
</section>
</section>
<section id="correlation-of-error-terms">
<h2>Correlation of error terms<a class="headerlink" href="#correlation-of-error-terms" title="Permalink to this headline">#</a></h2>
<p>Another important assumption of the linear regression model is that the error terms are uncorrelated. If they are not, then p-values associated with the model will be lower than they should be and confidence intervalls are not reliable <span id="id8">[<a class="reference internal" href="reference.html#id7" title="Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical learning. Springer, 2021. URL: https://www.statlearning.com/.">James <em>et al.</em>, 2021</a>]</span>.</p>
<p>Correlated errors occur especially in the context of time series data. In order to determine if this is the case for a given data set, we can plot the residuals from our model as a function of time. If the errors are uncorrelated, then there should be no evident pattern. This means if the errors are uncorrelated, then the fact that <span class="math notranslate nohighlight">\(ϵ_i\)</span> is positive provides little or no information about the sign of <span class="math notranslate nohighlight">\(ϵ_{i+1}\)</span>.</p>
<p>Correlation among the error terms can also occur outside of time series data. For instance, consider a study in which individuals’ heights are predicted from their weights. The assumption of uncorrelated errors could be violated if some of the individuals in the study are members of the same family, or eat the same diet, or have been exposed to the same environmental factors <span id="id9">[<a class="reference internal" href="reference.html#id3" title="Andy Field. Discovering statistics using IBM SPSS statistics. sage, 2013.">Field, 2013</a>]</span>.</p>
<section id="durbin-watson-test">
<h3>Durbin-Watson test<a class="headerlink" href="#durbin-watson-test" title="Permalink to this headline">#</a></h3>
<p>A test of autocorrelation that is designed to take account of the regression model is the <strong>Durbin-Watson test</strong> (also included in the regression summary). It is used to test the hypothesis that there is no <strong>lag one autocorrelation</strong> in the residuals. This means if there is no lag one autocorrelation, then information about <span class="math notranslate nohighlight">\(ϵ_i\)</span> provides little or no information about <span class="math notranslate nohighlight">\(ϵ_{i+1}\)</span>.</p>
<p>If there is no autocorrelation, the test-statistic is around 2.</p>
<ul class="simple">
<li><p>This statistic will always be between 0 and 4.</p></li>
<li><p>The closer to 0 the statistic, the more evidence for positive serial correlation.</p></li>
<li><p>The closer to 4, the more evidence for negative serial correlation.</p></li>
</ul>
<p>As a rough rule of thumb <span id="id10">[<a class="reference internal" href="reference.html#id3" title="Andy Field. Discovering statistics using IBM SPSS statistics. sage, 2013.">Field, 2013</a>]</span>:</p>
<ul class="simple">
<li><p>If Durbin–Watson is less than 1.0, there may be cause for concern.</p></li>
<li><p>Small values of d indicate successive error terms are positively correlated.</p></li>
<li><p>If d &gt; 2, successive error terms are negatively correlated.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sm</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">durbin_watson</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id11">
<h3>Solution<a class="headerlink" href="#id11" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>We can get consistent estimates of the standard errors with <a class="reference external" href="https://en.wikipedia.org/wiki/Newey%E2%80%93West_estimator">Newey-West standard errors</a>. See <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.stats.sandwich_covariance.cov_hac.html">statsmodels documentation</a> for implementation details.</p></li>
<li><p>Furthermore, you can use a <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.GLS.html">generalized least squares (GLS)</a> model instead of the ordinary least squares (OLS).</p></li>
</ul>
<p>In the case of correlated errors, you may also use a multilevel model (also known as hierarchical linear models, linear mixed-effect model, mixed models, nested data models, random coefficient, random-effects models, random parameter models, or split-plot designs).</p>
<p>Multilevel models are particularly appropriate for research designs where data for participants are organized at more than one level (i.e., nested data). The units of analysis are usually individuals (at a lower level) who are nested within contextual/aggregate units (at a higher level).</p>
<p>See <a class="reference external" href="https://www.statsmodels.org/stable/mixed_linear.html">statsmodels’ documentation</a> to learn how to implement multilevel models.</p>
</section>
</section>
<section id="collinearity">
<h2>Collinearity<a class="headerlink" href="#collinearity" title="Permalink to this headline">#</a></h2>
<p>Collinearity refers to the situation in which two or more predictor variables collinearity are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.</p>
<p>It is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation <strong>multicollinearity</strong>.</p>
<section id="correlation-matrix">
<h3>Correlation matrix<a class="headerlink" href="#correlation-matrix" title="Permalink to this headline">#</a></h3>
<p>A simple way to detect collinearity is to look at the <strong>correlation matrix</strong> of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.</p>
<p>Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix since it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inspect correlation</span>
<span class="c1"># Calculate correlation using the default method ( &quot;pearson&quot;)</span>
<span class="n">corr</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="c1"># optimize aesthetics: generate mask for removing duplicate / unnecessary info</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">corr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">triu_indices_from</span><span class="p">(</span><span class="n">mask</span><span class="p">)]</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># Generate a custom diverging colormap as indicator for correlations:</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">diverging_palette</span><span class="p">(</span><span class="mi">220</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Plot</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">});</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="variance-inflation-factor-vif">
<h3>Variance inflation factor (VIF)<a class="headerlink" href="#variance-inflation-factor-vif" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Instead of inspecting the correlation matrix, a better way to assess <strong>multicollinearity</strong> is to compute the variance inflation factor (VIF).</p></li>
<li><p>Note that we ignore the intercept in this test.</p></li>
</ul>
<ul class="simple">
<li><p>The smallest possible value for VIF is 1, which indicates the complete absence of collinearity.</p></li>
<li><p>Typically in practice there is a small amount of collinearity among the predictors.</p></li>
<li><p>As a rule of thumb, a VIF value that exceeds 5 indicates a problematic amount of collinearity and the parameter estimates will have large standard errors because of this.</p></li>
</ul>
<ul class="simple">
<li><p>Note that the function <code class="docutils literal notranslate"><span class="pre">variance_inflation_factor</span></code> expects the presence of a constant in the matrix of explanatory variables.</p></li>
<li><p>Therefore, we use <code class="docutils literal notranslate"><span class="pre">add_constant</span></code> from statsmodels to add the required constant to the dataframe before passing its values to the function.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># choose features and add constant</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">___</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;income&#39;</span><span class="p">,</span> <span class="s1">&#39;education&#39;</span><span class="p">]])</span>

<span class="c1"># create empty DataFrame</span>
<span class="n">vif</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="c1"># calculate vif</span>
<span class="n">vif</span><span class="p">[</span><span class="s2">&quot;VIF Factor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

<span class="c1"># add feature names</span>
<span class="n">vif</span><span class="p">[</span><span class="s2">&quot;Feature&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">columns</span>

<span class="n">vif</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We don’t have a problematic amount of collinearity in our data.</p>
</section>
<section id="id12">
<h3>Solution<a class="headerlink" href="#id12" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A simple solution would be to remove some of the highly correlated features.</p></li>
<li><p>Furthermore, you could manually combine some features (e.g. adding them together) or use a method which automatically combines features, such as principal components analysis or partial least squares regression.</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Jan Kirenz<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>